{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/20_newsgroups/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(data_dir):\n",
    "    # Assign target values to each of the classes in the dataset\n",
    "    targets = {}\n",
    "    for i, newsgroup in enumerate(os.listdir(data_dir)):\n",
    "        targets[newsgroup] = i\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning a target value to each document class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 0,\n",
       " 'rec.autos': 1,\n",
       " 'comp.windows.x': 2,\n",
       " 'sci.med': 3,\n",
       " 'sci.crypt': 4,\n",
       " 'comp.os.ms-windows.misc': 5,\n",
       " 'talk.politics.mideast': 6,\n",
       " 'talk.politics.misc': 7,\n",
       " 'sci.electronics': 8,\n",
       " 'rec.sport.baseball': 9,\n",
       " 'rec.sport.hockey': 10,\n",
       " 'comp.graphics': 11,\n",
       " 'sci.space': 12,\n",
       " 'talk.politics.guns': 13,\n",
       " 'comp.sys.mac.hardware': 14,\n",
       " 'misc.forsale': 15,\n",
       " 'talk.religion.misc': 16,\n",
       " 'rec.motorcycles': 17,\n",
       " 'comp.sys.ibm.pc.hardware': 18,\n",
       " 'soc.religion.christian': 19}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_dict = get_targets(data_dir)\n",
    "targets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_paths(data_dir):\n",
    "    X_paths, Y = [], []\n",
    "    targets_dict = get_targets(data_dir)\n",
    "    for newsgroup_dir in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, newsgroup_dir)\n",
    "        for text_file in os.listdir(class_path):\n",
    "            file_path = os.path.join(class_path, text_file)\n",
    "            try:\n",
    "                with open(file_path, 'r') as fp:\n",
    "                    x = fp.readlines()\n",
    "            except UnicodeDecodeError:\n",
    "                print(f'DecodeError, ignoring -- {file_path}')\n",
    "                os.remove(file_path)\n",
    "                continue\n",
    "            X_paths.append(file_path)\n",
    "            Y.append(targets_dict.get(newsgroup_dir))\n",
    "            \n",
    "    return X_paths, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_paths, Y = get_data_paths(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data samples: 19924\n"
     ]
    }
   ],
   "source": [
    "print(f'Total data samples: {len(Y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly checking if the data is correct or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/20_newsgroups/misc.forsale/74749',\n",
       " 'data/20_newsgroups/talk.politics.guns/53324',\n",
       " 'data/20_newsgroups/talk.politics.misc/178729',\n",
       " 'data/20_newsgroups/comp.graphics/38571',\n",
       " 'data/20_newsgroups/comp.os.ms-windows.misc/10072']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(X_paths, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 19, 7, 11, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(Y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(X, y, test_pct=0.5):\n",
    "    total_len = len(y)\n",
    "    train_len = int(test_pct*total_len)\n",
    "    train_indices = random.sample(range(total_len), train_len)\n",
    "    test_indices = [k for k in range(total_len) if k not in train_indices]\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "    for i in train_indices:\n",
    "        X_train.append(X[i])\n",
    "        y_train.append(y[i])\n",
    "        \n",
    "    for i in test_indices:\n",
    "        X_test.append(X[i])\n",
    "        y_test.append(y[i])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words taken from NLTK corpora\n",
    "- These words are very common and do not contribute much to the semantic meaning of a text document\n",
    "- So, I am filtering out these words from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
    "#  'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \n",
    "#  'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
    "#  'each', 'few', 'for', 'from', 'further', \n",
    "#  'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
    "#  'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",\n",
    "#  'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n",
    "#  \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    "#  'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
    "#  'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', \n",
    "#  'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \n",
    "#  \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', \n",
    "#  'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    "#  \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", \n",
    "#  'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \n",
    "#  'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n",
    "#  '4th', '5th', '6th', '7th', '8th', '9th', '10th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove headers from the document text\n",
    "- There are a couple of line breaks after header information in each file\n",
    "- So, check for that and remove everything above that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_headers(lines):\n",
    "    for i, line in enumerate(lines):\n",
    "        # First make sure that the bytecodes read is decoded\n",
    "        line = line.decode(encoding='utf-8')\n",
    "        if line == '\\n':\n",
    "            break\n",
    "    return lines[i+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove whitespaces and stop words from every line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(word):\n",
    "    for i in range(10):\n",
    "        word = word.replace(str(i), '')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(word):\n",
    "    all_punctuations = punctuation.replace(\"'\", \"\")\n",
    "    # Also, add tabs\n",
    "    all_punctuations += '\\t'\n",
    "    table = str.maketrans('', '', all_punctuations)\n",
    "    return word.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and applies some preprocessing\n",
    "    1. Remove numbers from string\n",
    "    2. Remove punctuations\n",
    "    3. Remove quotes from words if present\n",
    "    \"\"\"\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # Remove numbers from words\n",
    "        word = remove_digits(word)\n",
    "\n",
    "        # Remove punctuations\n",
    "        word = remove_punctuations(word)\n",
    "\n",
    "        # Do not process empty or one character strings\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "\n",
    "        # Also check for quoted words and remove the quotes\n",
    "        if word[0] in [\"'\", '\"']:\n",
    "            word = word[1:]\n",
    "        if word[-1] in [\"'\", '\"']:\n",
    "            word = word[:-1]\n",
    "            \n",
    "        processed_words.append(word)\n",
    "    \n",
    "    return processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_line(line):\n",
    "    # Return a list of valid words\n",
    "    words = line.replace('\\n', '').strip().split(' ')\n",
    "    words = pre_process(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            lines = file.readlines()\n",
    "        valid_lines = remove_headers(lines)\n",
    "        valid_words = []\n",
    "        for line in valid_lines:\n",
    "            # Decode byte words to string on each line\n",
    "            line = line.decode(encoding='utf-8')\n",
    "            processed_line = validate_line(line)\n",
    "            for word in processed_line:\n",
    "                word = word.lower()\n",
    "                if len(word) > 1 and word not in stop_words:\n",
    "                    valid_words.append(word)\n",
    "                    \n",
    "    except Exception as error:\n",
    "        # print(f'ERROR: {error} || FILE_NAME: {file_path}')\n",
    "        return [], 1\n",
    "    \n",
    "    return valid_words, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article',\n",
       " 'cvmrzdarksideosrheuoknoredu',\n",
       " 'bilokcforumosrheedu',\n",
       " 'conner',\n",
       " 'writes',\n",
       " 'myth',\n",
       " 'refer',\n",
       " 'convoluted',\n",
       " 'counterfeit',\n",
       " 'athiests']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file('data/20_newsgroups/alt.atheism/54238')[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(X, n_features=4000, reject_words=0):\n",
    "    \"\"\"Goes through the entire training set and gets top \"n_features\" words appeared in the documents along with their frequencies\"\"\"\n",
    "    all_words = []\n",
    "    file_errors = 0\n",
    "    for file_path in X:\n",
    "        words, has_error = read_file(file_path)\n",
    "        file_errors += has_error\n",
    "        for w in words:\n",
    "            all_words.append(w)\n",
    "            \n",
    "    words, counts = np.unique(np.array(all_words), return_counts=True)\n",
    "    freq, words = (list(i) for i in zip(*sorted(zip(counts, words), reverse=True)))\n",
    "    # print(len(words), words[:10], freq[:10])\n",
    "    # print(f'Total file encoding errors: {file_errors}')\n",
    "    \n",
    "    # Return the 4000 words removing the first reject_words (as they are very common and won't be useful in differentiating among the documents)\n",
    "    # in the whole dataset\n",
    "    return words[reject_words:n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_word_freq(X):\n",
    "    \"\"\"\n",
    "        Returns a list of dictionaries that contain the frequencies of words in each document\n",
    "        --> [{'word1': 3, ...} ...]\n",
    "    \"\"\"\n",
    "    word_freq = []\n",
    "    for file_path in X:\n",
    "        words, has_error = read_file(file_path)\n",
    "        words, counts = np.unique(np.array(words), return_counts=True)\n",
    "        word_counts = {}\n",
    "        for i, word in enumerate(words):\n",
    "            word_counts[word] = counts[i]\n",
    "        \n",
    "        word_freq.append(word_counts)\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(X, feature_words):\n",
    "    X_data = []\n",
    "    word_freq = doc_word_freq(X)\n",
    "    for doc_words in word_freq:\n",
    "        # doc_words is a dict that contains words in that document along with their number of appearences\n",
    "        doc_data = []\n",
    "        for f_word in feature_words:\n",
    "            if f_word in doc_words.keys():\n",
    "                # Add the frequency for the word to create a feature vector for training set\n",
    "                doc_data.append(doc_words[f_word])\n",
    "            else:\n",
    "                doc_data.append(0)\n",
    "        X_data.append(doc_data)\n",
    "    return np.array(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(X_train, y_train, X_test, y_test, n_features=4000, reject_words=0):\n",
    "    feature_words = get_features(X_train, n_features, reject_words)\n",
    "    X_train = create_data(X_train, feature_words)\n",
    "    X_test = create_data(X_test, feature_words)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    print(f'Train samples: {len(X_train)} || Test samples: {len(X_test)}')\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.prior = None\n",
    "        self._is_trained = False\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        n = X_train.shape[0]\n",
    "        \n",
    "        # Separate data in X_train by its classes\n",
    "        X_by_class = np.array([X_train[y_train == c] for c in np.unique(y_train)])\n",
    "        self.prior = np.array([len(X_class)/n for X_class in X_by_class])\n",
    "        \n",
    "        # Get word counts\n",
    "        self.word_counts = np.array([row.sum(axis=0) for row in X_by_class]) + self.alpha\n",
    "        self.lk_word = self.word_counts / self.word_counts.sum(axis=1).reshape(-1, 1)\n",
    "        \n",
    "        self._is_trained = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).argmax(axis=1)\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        y_pred = self.predict_prob(X_test).argmax(axis=1)\n",
    "        return np.mean(y_pred == y_test)\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if not self._is_trained:\n",
    "            print('Model not trained yet!!')\n",
    "            \n",
    "        # Go through each input vector to calculate the conditional probabilities\n",
    "        class_nums = np.zeros(shape=(X.shape[0], self.prior.shape[0]))\n",
    "        for i, x in enumerate(X):\n",
    "            word_exists = x.astype(bool)\n",
    "            lk_words_present = self.lk_word[:, word_exists] ** x[word_exists]\n",
    "            lk_message = (lk_words_present).prod(axis=1)\n",
    "            class_nums[i] = lk_message * self.prior\n",
    "            \n",
    "        normalize_term = class_nums.sum(axis=1).reshape(-1, 1)\n",
    "        conditional_probs = class_nums / normalize_term\n",
    "        return conditional_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(X_paths, Y, k=5, n_features=5000, increment=1000, reject_words=0):\n",
    "    for i in range(k):\n",
    "        print(f'\\nfold: {i} || n_features: {n_features} || reject_words: {reject_words}')\n",
    "        X_train_list, y_train_list, X_test_list, y_test_list = split_train_test(X_paths, Y, test_pct=0.5)\n",
    "        X_train, y_train, X_test, y_test = get_train_test(X_train_list, y_train_list, X_test_list, y_test_list, n_features, reject_words)\n",
    "        alpha = random.uniform(0.5, 1.0)\n",
    "        print(f'Alpha chosen: {alpha}')\n",
    "        clf = NaiveBayes(alpha=alpha)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(f'Acc: {clf.score(X_test, y_test)}')\n",
    "        #n_features += increment    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold: 0 || n_features: 5000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.8131513439076901\n",
      "Acc: 0.6844007227464365\n",
      "\n",
      "fold: 1 || n_features: 6000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.8182395813367933\n",
      "Acc: 0.6923308572575788\n",
      "\n",
      "fold: 2 || n_features: 7000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.7050027565002497\n",
      "Acc: 0.7020678578598675\n",
      "\n",
      "fold: 3 || n_features: 8000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.8772637911906764\n",
      "Acc: 0.6997590845211805\n",
      "\n",
      "fold: 4 || n_features: 9000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.7751594113962157\n",
      "Acc: 0.7087934149769123\n",
      "\n",
      "fold: 5 || n_features: 10000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.8205057389889763\n",
      "Acc: 0.7035735796024895\n",
      "\n",
      "fold: 6 || n_features: 11000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.8617624079385859\n",
      "Acc: 0.7046777755470789\n",
      "\n",
      "fold: 7 || n_features: 12000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.7138341043845708\n",
      "Acc: 0.7074884561333066\n",
      "\n",
      "fold: 8 || n_features: 13000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.519860364482198\n",
      "Acc: 0.7136117245533026\n",
      "\n",
      "fold: 9 || n_features: 14000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.9691349812451657\n",
      "Acc: 0.7197349929732986\n"
     ]
    }
   ],
   "source": [
    "k_fold(X_paths, Y, k=10, reject_words=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold: 0 || n_features: 14000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.72379781297586\n",
      "Acc: 0.7114033326641237\n",
      "\n",
      "fold: 1 || n_features: 14000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.9437328497264166\n",
      "Acc: 0.7073880746837984\n",
      "\n",
      "fold: 2 || n_features: 14000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.5413693747693333\n",
      "Acc: 0.7186307970287091\n",
      "\n",
      "fold: 3 || n_features: 14000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.5612603118958358\n",
      "Acc: 0.7143143946998595\n",
      "\n",
      "fold: 4 || n_features: 14000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.5382103066097286\n",
      "Acc: 0.711704477012648\n",
      "\n",
      "fold: 5 || n_features: 14000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.669042708208162\n",
      "Acc: 0.7115037141136318\n",
      "\n",
      "fold: 6 || n_features: 14000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.9129900331480532\n",
      "Acc: 0.7074884561333066\n",
      "\n",
      "fold: 7 || n_features: 14000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.725490893816866\n",
      "Acc: 0.71321019875527\n",
      "\n",
      "fold: 8 || n_features: 14000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.6191943269659299\n",
      "Acc: 0.7115037141136318\n",
      "\n",
      "fold: 9 || n_features: 14000 || reject_words: 900\n",
      "Train samples: 9962 || Test samples: 9962\n",
      "Alpha chosen: 0.79587695498053\n",
      "Acc: 0.7084922706283878\n"
     ]
    }
   ],
   "source": [
    "k_fold(X_paths, Y, k=10, n_features=14000, reject_words=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 9962 || Test samples: 9962\n"
     ]
    }
   ],
   "source": [
    "X_train_list, y_train_list, X_test_list, y_test_list = split_train_test(X_paths, Y, test_pct=0.5)\n",
    "X_train, y_train, X_test, y_test = get_train_test(X_train_list, y_train_list, X_test_list, y_test_list, n_features=14000, reject_words=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782975306163421\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asheesh",
   "language": "python",
   "name": "asheesh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
