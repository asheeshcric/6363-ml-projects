{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/20_newsgroups/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(data_dir):\n",
    "    # Assign target values to each of the classes in the dataset\n",
    "    targets = {}\n",
    "    for i, newsgroup in enumerate(os.listdir(data_dir)):\n",
    "        targets[newsgroup] = i\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning a target value to each document class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 0,\n",
       " 'rec.autos': 1,\n",
       " 'comp.windows.x': 2,\n",
       " 'sci.med': 3,\n",
       " 'sci.crypt': 4,\n",
       " 'comp.os.ms-windows.misc': 5,\n",
       " 'talk.politics.mideast': 6,\n",
       " 'talk.politics.misc': 7,\n",
       " 'sci.electronics': 8,\n",
       " 'rec.sport.baseball': 9,\n",
       " 'rec.sport.hockey': 10,\n",
       " 'comp.graphics': 11,\n",
       " 'sci.space': 12,\n",
       " 'talk.politics.guns': 13,\n",
       " 'comp.sys.mac.hardware': 14,\n",
       " 'misc.forsale': 15,\n",
       " 'talk.religion.misc': 16,\n",
       " 'rec.motorcycles': 17,\n",
       " 'comp.sys.ibm.pc.hardware': 18,\n",
       " 'soc.religion.christian': 19}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_dict = get_targets(data_dir)\n",
    "targets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_paths(data_dir):\n",
    "    X_paths, Y = [], []\n",
    "    targets_dict = get_targets(data_dir)\n",
    "    for newsgroup_dir in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, newsgroup_dir)\n",
    "        for text_file in os.listdir(class_path):\n",
    "            file_path = os.path.join(class_path, text_file)\n",
    "            try:\n",
    "                with open(file_path, 'r') as fp:\n",
    "                    x = fp.readlines()\n",
    "            except UnicodeDecodeError:\n",
    "                print(f'DecodeError, deleting -- {file_path}')\n",
    "                continue\n",
    "            X_paths.append(file_path)\n",
    "            Y.append(targets_dict.get(newsgroup_dir))\n",
    "            \n",
    "    return X_paths, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecodeError, deleting -- data/20_newsgroups/alt.atheism/52499\n",
      "DecodeError, deleting -- data/20_newsgroups/alt.atheism/52910\n",
      "DecodeError, deleting -- data/20_newsgroups/alt.atheism/52909\n",
      "DecodeError, deleting -- data/20_newsgroups/alt.atheism/51060\n",
      "DecodeError, deleting -- data/20_newsgroups/alt.atheism/54164\n",
      "DecodeError, deleting -- data/20_newsgroups/alt.atheism/49960\n",
      "DecodeError, deleting -- data/20_newsgroups/alt.atheism/54163\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.autos/103700\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.autos/103694\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.autos/103725\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.autos/101596\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.windows.x/68001\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.windows.x/67305\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.windows.x/66871\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.windows.x/66923\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.med/59055\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.med/59535\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.med/59239\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.crypt/15672\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.electronics/53883\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.electronics/54485\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.electronics/53803\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.electronics/54071\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.electronics/54070\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.electronics/53721\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.baseball/104984\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.baseball/104352\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.baseball/104572\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.baseball/104419\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.baseball/104471\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.baseball/104474\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.baseball/104562\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.hockey/54516\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.hockey/54036\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.hockey/54769\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.hockey/53879\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.hockey/54000\n",
      "DecodeError, deleting -- data/20_newsgroups/rec.sport.hockey/54042\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.graphics/38568\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.graphics/38757\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.graphics/38489\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.graphics/38291\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.graphics/38490\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.space/61556\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.space/61293\n",
      "DecodeError, deleting -- data/20_newsgroups/sci.space/61534\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/51917\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/51916\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/52164\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/51594\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/50467\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/51592\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/52196\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/51892\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/51591\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/51941\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/51593\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/51865\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/52033\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/51904\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.mac.hardware/52165\n",
      "DecodeError, deleting -- data/20_newsgroups/misc.forsale/76361\n",
      "DecodeError, deleting -- data/20_newsgroups/misc.forsale/76843\n",
      "DecodeError, deleting -- data/20_newsgroups/misc.forsale/76548\n",
      "DecodeError, deleting -- data/20_newsgroups/misc.forsale/76112\n",
      "DecodeError, deleting -- data/20_newsgroups/talk.religion.misc/82776\n",
      "DecodeError, deleting -- data/20_newsgroups/talk.religion.misc/83651\n",
      "DecodeError, deleting -- data/20_newsgroups/talk.religion.misc/83777\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.ibm.pc.hardware/60498\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.ibm.pc.hardware/61116\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.ibm.pc.hardware/60366\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.ibm.pc.hardware/60936\n",
      "DecodeError, deleting -- data/20_newsgroups/comp.sys.ibm.pc.hardware/60868\n"
     ]
    }
   ],
   "source": [
    "X_paths, Y = get_data_paths(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data samples: 19924\n"
     ]
    }
   ],
   "source": [
    "print(f'Total data samples: {len(Y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly checking if the data is correct or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/20_newsgroups/talk.politics.mideast/77293',\n",
       " 'data/20_newsgroups/alt.atheism/54168',\n",
       " 'data/20_newsgroups/sci.med/59452',\n",
       " 'data/20_newsgroups/rec.autos/102943',\n",
       " 'data/20_newsgroups/sci.electronics/53577']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(X_paths, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 10, 14, 8, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(Y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(X, y, test_pct=0.5):\n",
    "    total_len = len(y)\n",
    "    train_len = int(test_pct*total_len)\n",
    "    train_indices = random.sample(range(total_len), train_len)\n",
    "    test_indices = [k for k in range(total_len) if k not in train_indices]\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "    for i in train_indices:\n",
    "        X_train.append(X[i])\n",
    "        y_train.append(y[i])\n",
    "        \n",
    "    for i in test_indices:\n",
    "        X_test.append(X[i])\n",
    "        y_test.append(y[i])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(X_paths, Y, test_pct=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 9962 || Testing samples: 9962\n"
     ]
    }
   ],
   "source": [
    "print(f'Training samples: {len(y_train)} || Testing samples: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words taken from NLTK corpora\n",
    "- These words are very common and do not contribute much to the semantic meaning of a text document\n",
    "- So, I am filtering out these words from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
    " 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \n",
    " 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
    " 'each', 'few', 'for', 'from', 'further', \n",
    " 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
    " 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",\n",
    " 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n",
    " \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    " 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
    " 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', \n",
    " 'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \n",
    " \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', \n",
    " 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    " \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", \n",
    " 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \n",
    " 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n",
    " '4th', '5th', '6th', '7th', '8th', '9th', '10th']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove headers from the document text\n",
    "- There are a couple of line breaks after header information in each file\n",
    "- So, check for that and remove everything above that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_headers(lines):\n",
    "    for i, line in enumerate(lines):\n",
    "        # First make sure that the bytecodes read is decoded\n",
    "        line = line.decode(encoding='utf-8')\n",
    "        if line == '\\n':\n",
    "            break\n",
    "    return lines[i+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove whitespaces and stop words from every line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(word):\n",
    "    for i in range(10):\n",
    "        word = word.replace(str(i), '')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(word):\n",
    "    all_punctuations = punctuation.replace(\"'\", \"\")\n",
    "    # Also, add tabs\n",
    "    all_punctuations += '\\t'\n",
    "    table = str.maketrans('', '', all_punctuations)\n",
    "    return word.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and applies some preprocessing\n",
    "    1. Remove numbers from string\n",
    "    2. Remove punctuations\n",
    "    3. Remove quotes from words if present\n",
    "    \"\"\"\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # Remove numbers from words\n",
    "        word = remove_digits(word)\n",
    "\n",
    "        # Remove punctuations\n",
    "        word = remove_punctuations(word)\n",
    "\n",
    "        # Do not process empty or one character strings\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "\n",
    "        # Also check for quoted words and remove the quotes\n",
    "        if word[0] in [\"'\", '\"']:\n",
    "            word = word[1:]\n",
    "        if word[-1] in [\"'\", '\"']:\n",
    "            word = word[:-1]\n",
    "            \n",
    "        processed_words.append(word)\n",
    "    \n",
    "    return processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    # Remove stop words from a list of words\n",
    "    # Also, remove empty strings and single char strings\n",
    "    return [word.lower() for word in words if len(word)>1 and word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_line(line):\n",
    "    # Return a list of valid words\n",
    "    words = line.replace('\\n', '').strip().split(' ')\n",
    "    words = pre_process(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            lines = file.readlines()\n",
    "        valid_lines = remove_headers(lines)\n",
    "        valid_words = []\n",
    "        for line in valid_lines:\n",
    "            # Decode byte words to string on each line\n",
    "            line = line.decode(encoding='utf-8')\n",
    "            processed_line = validate_line(line)\n",
    "            for word in processed_line:\n",
    "                word = word.lower()\n",
    "                if len(word) > 1 and word not in stop_words:\n",
    "                    valid_words.append(word)\n",
    "                    \n",
    "    except Exception as error:\n",
    "        # print(f'ERROR: {error} || FILE_NAME: {file_path}')\n",
    "        return [], 1\n",
    "    \n",
    "    return valid_words, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['article',\n",
       "  'cvmrzdarksideosrheuoknoredu',\n",
       "  'bilokcforumosrheedu',\n",
       "  'bill',\n",
       "  'conner',\n",
       "  'writes',\n",
       "  'myth',\n",
       "  'refer',\n",
       "  'convoluted',\n",
       "  'counterfeit',\n",
       "  'athiests',\n",
       "  'created',\n",
       "  'make',\n",
       "  'religion',\n",
       "  'appear',\n",
       "  'absurd',\n",
       "  'rather',\n",
       "  'approach',\n",
       "  'religion',\n",
       "  'including',\n",
       "  'christainity',\n",
       "  'rational',\n",
       "  'manner',\n",
       "  'debating',\n",
       "  'claims',\n",
       "  'stated',\n",
       "  'atheists',\n",
       "  'concoct',\n",
       "  'outrageous',\n",
       "  'parodies',\n",
       "  'hold',\n",
       "  'religious',\n",
       "  'accountable',\n",
       "  'beliefs',\n",
       "  'accurately',\n",
       "  'oxymoric',\n",
       "  'term',\n",
       "  'like',\n",
       "  'reasonable',\n",
       "  'atheist',\n",
       "  'religious',\n",
       "  'parodies',\n",
       "  'atheistic',\n",
       "  'paradies',\n",
       "  'please',\n",
       "  'substantiate',\n",
       "  'parodies',\n",
       "  'outrageous',\n",
       "  'specifically',\n",
       "  'iup',\n",
       "  'outrageous',\n",
       "  'many',\n",
       "  'religions',\n",
       "  'private',\n",
       "  'note',\n",
       "  'jennifer',\n",
       "  'fakult',\n",
       "  'post',\n",
       "  'may',\n",
       "  'contain',\n",
       "  'following',\n",
       "  'sarcasm',\n",
       "  'cycnicism',\n",
       "  'irony',\n",
       "  'humor',\n",
       "  'please',\n",
       "  'aware',\n",
       "  'possibility',\n",
       "  'allow',\n",
       "  'confused',\n",
       "  'andor',\n",
       "  'thrown',\n",
       "  'loop',\n",
       "  'doubt',\n",
       "  'assume',\n",
       "  'owners',\n",
       "  'account',\n",
       "  'take',\n",
       "  'responsiblity',\n",
       "  'confusion',\n",
       "  'may',\n",
       "  'result',\n",
       "  'inability',\n",
       "  'recognize',\n",
       "  'read',\n",
       "  'risk',\n",
       "  'jennifer'],\n",
       " 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file('data/20_newsgroups/alt.atheism/54238')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(X, n_features=4000):\n",
    "    \"\"\"Goes through the entire training set and gets top \"n_features\" words appeared in the documents along with their frequencies\"\"\"\n",
    "    all_words = []\n",
    "    file_errors = 0\n",
    "    for file_path in X:\n",
    "        words, has_error = read_file(file_path)\n",
    "        file_errors += has_error\n",
    "        for w in words:\n",
    "            all_words.append(w)\n",
    "            \n",
    "    words, counts = np.unique(np.array(all_words), return_counts=True)\n",
    "    freq, words = (list(i) for i in zip(*sorted(zip(counts, words), reverse=True)))\n",
    "    print(len(words), words[:10], freq[:10])\n",
    "    print(f'Total file encoding errors: {file_errors}')\n",
    "    \n",
    "    # Return the top 4000 words in the whole dataset\n",
    "    return words[:n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101128 ['writes', 'article', 'like', 'people', 'just', 'know', 'get', 'think', 'also', 'use'] [7331, 6247, 4935, 4877, 4821, 4407, 4129, 3958, 3635, 3252]\n",
      "Total file encoding errors: 0\n"
     ]
    }
   ],
   "source": [
    "feature_words = get_features(X_train, n_features=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_word_freq(X):\n",
    "    \"\"\"\n",
    "        Returns a list of dictionaries that contain the frequencies of words in each document\n",
    "        --> [{'word1': 3, ...} ...]\n",
    "    \"\"\"\n",
    "    word_freq = []\n",
    "    for file_path in X:\n",
    "        words, has_error = read_file(file_path)\n",
    "        words, counts = np.unique(np.array(words), return_counts=True)\n",
    "        word_counts = {}\n",
    "        for i, word in enumerate(words):\n",
    "            word_counts[word] = counts[i]\n",
    "        \n",
    "        word_freq.append(word_counts)\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(X, feature_words):\n",
    "    X_data = []\n",
    "    word_freq = doc_word_freq(X)\n",
    "    for doc_words in word_freq:\n",
    "        # doc_words is a dict that contains words in that document along with their number of appearences\n",
    "        doc_data = []\n",
    "        for f_word in feature_words:\n",
    "            if f_word in doc_words.keys():\n",
    "                # Add the frequency for the word to create a feature vector for training set\n",
    "                doc_data.append(doc_words[f_word])\n",
    "            else:\n",
    "                doc_data.append(0)\n",
    "        X_data.append(doc_data)\n",
    "    return np.array(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = create_data(X_train, feature_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_np = create_data(X_test, feature_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9962 9962\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_np), len(X_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asheesh",
   "language": "python",
   "name": "asheesh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
